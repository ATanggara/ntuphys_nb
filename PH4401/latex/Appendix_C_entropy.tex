\documentclass[pra,12pt]{revtex4}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{color}
\usepackage[pdfborder={0 0 0},colorlinks=true,linkcolor=blue,urlcolor=blue]{hyperref}

\def\ket#1{\left|#1\right\rangle}
\def\bra#1{\left\langle#1\right|}
\def\braket#1{\left\langle#1\right\rangle}

\usepackage{fancyhdr}
\fancyhf{}
\lhead{\tiny Y.~D.~Chong (2018)}
\rhead{\scriptsize PH4401: Quantum Mechanics III}
\lfoot{}
\rfoot{\thepage}
\pagestyle{fancy}

\setlength{\parindent}{0pt}

\renewcommand{\baselinestretch}{1.0}
\setlength{\parskip}{0.07in}

\begin{document}

\begin{center}
{\large \textbf{Appendix C: Entropy}}
\end{center}

``Entropy'' is an important theoretical concept, used in multiple
fields of science and mathematics, that provides a way to quantify
one's lack of knowledge about a complex system.  In thermodynamics and
statistical mechanics, the quantity called \textbf{thermodynamic
  entropy} describes our uncertainty about the precise microscopic
details, or ``microstate'', of a many-body system.  In the
mathematical field of information theory, the quantity called
\textbf{information entropy}, also called \textbf{Shannon entropy}
after its inventor Claude Shannon, describes the uncertainty about the
contents of a transmitted message.  One of the most profound
accomplishments of theoretical physics in the 20th century was the
discovery, by E.~T.~Jaynes in 1957, that statistical mechanics can be
reformulated in terms of information theory.  In light of Jaynes'
work, the thermodynamics-based and information-based concepts of
entropy are one and the same.  For details about this fascinating
topic, see \hyperref[cite:jaynes]{Jaynes (1957)} and
\hyperref[cite:jaynes2]{Jaynes (1957a)}.  In this appendix, we will
summarize the definition of entropy in classical physics, and how it
is related to other thermodynamic quantities.

Suppose a system has discrete microstates labeled by integers
$\{1,2,3,\dots\}$ , which have probabilities $\{p_1, p_2, p_3,
\dots\}$.  Then the entropy of the system is defined as
$$S_{\mathrm{cl.}} = - k_B \sum_{i} p_i \ln(p_i).$$
The probabilities $p_i$ are \textit{conditional} probabilities.  They
are conditioned on the set of known facts about the macroscopic
features of the system (e.g., in some cases we might know the total
energy $E$).

Consider the behavior of $S_{\mathrm{cl.}}$ under two extreme
scenarios:

In the first scenario, we know the exact microstate of the system
(i.e., $p_k = 1$ for some $k$).  In this case, the entropy formula
gives $S_{\mathrm{cl.}} = 0$.

In the second scenario, we are in a state of ``complete uncertainty''.
This can happen if the system is at equilibrium, with a fixed total
energy $E$, and does not interact with the rest of the universe.  For
such a system, which is called a \textbf{micro-canonical ensemble},
the \textbf{ergodicity postulate} of statistical mechanics states that
all possible microstates with energy $E$ are equally probable.  If
there are $W$ possible microstates, the probabilities are
$$p_i = \frac{1}{W} \;\;\forall i \in \{1,2,\dots,W\}.$$
Therefore, the entropy formula gives
$$S_{\mathrm{cl.}} \,=\, -k_B W \frac{1}{W} \ln(1/W) \;=\; k_B \ln W,$$
the famous result carved into the gravestone of Ludwig Boltzmann.
Note that this expression has an implicit energy dependence: changing
$E$ varies $W$ and hence $S_{\mathrm{cl.}}$.

The entropy formula is designed so that any other probability
distribution (which necessarily refers to a situation of
\textit{partial} uncertainty) yields an entropy $S_{\mathrm{cl.}}$
lying between $0$ and $k_B \ln W$.  To see that zero is the lower
bound, first note that for $0 \le p_i \le 1$, each term in the entropy
formula satisfies $-k_B\, p_i\ln(p_i) \ge 0$, and the equality holds
if and only if $p_i = 0$ or $p_i = 1$.  See the figure below:

\begin{figure}[h]
  \centering\includegraphics[width=0.55\textwidth]{plogp}
\end{figure}

This implies that $S_{\mathrm{cl.}}\ge 0$, and moreover that
$S_{\mathrm{cl.}} = 0$ if and only if $p_i = \delta_{ik}$ for some $k$
(i.e., there is no uncertainty).  Next, it can be shown that
$S_{\mathrm{cl.}}$ is bounded above by $k_B \ln W$.  This statement is
equivalent to the \textbf{second law of thermodynamics}; we will not
go over the details of the proof, but it follows from a mathematical
relation known as
\href{https://en.wikipedia.org/wiki/Gibbs\%27_inequality}{Gibbs'
  inequality}.

Another important feature of the entropy is that it is
\textbf{extensive}, meaning that it scales (``extends'')
proportionally with system size.  To see this, consider two
independent systems A and B, which have microstate probabilities
$\{p_i^A\}$ and $\{p_j^B\}$.  If we regard the combination of A and B
as a single system, each microstate of the combined system is
specified by the microstate of A and the microstate of B, and is thus
indexed by integers $(i,j)$, with probability $p_{ij} = p^A_ip^B_j$.
The entropy of the combined system is
$$\begin{aligned}S_{\mathrm{cl.}} &= - k_B \sum_{ij} p_i^Ap^B_j \ln\left(p^A_ip^B_j\right) \\
  &= - k_B \Big(\sum_{i} p^A_i \ln p^A_i\Big)\Big(\sum_j p^B_j\Big)
  - k_B \Big(\sum_{i} p^A_i \Big) \Big(\sum_j p^B_j \ln p^B_j\Big) \\
  &= S_{\mathrm{cl.}}^A + S_{\mathrm{cl.}}^B,
\end{aligned}$$
where $S_{\mathrm{cl.}}^A$ and $S_{\mathrm{cl.}}^B$ are the individual
entropies of the A and B subsystems.

Extensivity has important consequences for the behavior of $W(E)$, the
number of microstates at each energy $E$.  Suppose we extend a system
by adding micro-canonical subsystems (which are insulated from each
other).  In the process, both $E$ and $S_{\mathrm{cl.}}$ increase
proportionally.  Since $S_{\mathrm{cl.}} \propto \ln[W(E)]$,
$$E \propto \ln W \;\;\;\Rightarrow \;\;\;W(E) \propto e^{\beta_0 E} \;\;\; \mathrm{for\;some}\; \beta_0 > 0.$$
If we relax the restriction that the additional subsystems are
micro-canonical, the number of microstates grows even faster with $E$,
as energy can now be distributed in different ways between the
subsystems.  It is reasonable to assume that the scaling is a
faster-growing exponential,
$$W(E) \propto e^{\beta E} \;\;\; \mathrm{for\;some}\; \beta \ge \beta_0.$$
This implies that the constant of proportionality relating
$S_{\mathrm{cl.}}$ and $E$ is
$$\frac{\partial E}{\partial S_{\mathrm{cl.}}} = \frac{1}{k_B \beta} = T,$$
where $\beta \equiv (k_BT)^{-1}$ \textit{defines} the temperature $T$.
This is the \textbf{first law of thermodynamics}.

A \textbf{canonical ensemble} is a system held in equilibrium with a
larger system, called a ``heat bath''.  We can model this as a
micro-canonical ensemble of energy $E$, divided into two interacting
subsystems, A (the canonical ensemble) and B (the heat bath).  Using
the ergodicity postulate, and the aforementioned exponential scaling
of $W$ with $E$, one can show that the probability for subsystem A to
have energy $E_A$ is
$$p_A(E_A) \propto W_A(E_A) \, e^{-\beta E_A},$$
where $W_A(E_A)$ is the number of microstates of energy $E_A$ for
subsystem A, and $\beta$ is the inverse temperature of the heat bath.
This is called the \textbf{Boltzmann law}.  It implies that each
microstate $i$, of energy $E_i$, has probability
$$p_i = \frac{\exp(-\beta E_i)}{Z}, \;\;\;\mathrm{where}\;\;Z \equiv \sum_j \exp(-\beta E_j).$$
$Z(\beta,E_1, E_2,\dots)$ is called the \textbf{partition function}.
Note that $p_i$ satisfies probability conservation, $\sum_i p_i = 1$,
and that the sum involves all microstates of all possible energies.

The probability distribution for a canonical ensemble represents a
partial-uncertainty situation, since lower-energy microstates are
more probable than higher-energy microstates.  Plugging the above
expression for $p_i$ into the entropy formula gives:
$$S_{\mathrm{cl.}} = \frac{1}{T} \frac{\sum_i E_i e^{-\beta E_i}}{\sum_i e^{-\beta E_i}} + k_B \ln Z \;=\; \frac{\langle E\rangle}{T} + k_B \ln Z,$$
where $\langle E\rangle = \sum_i E_i p_i$ denotes the average energy.
We can then define
$$F \,\equiv\, - k_B T \ln Z \,=\, \langle E \rangle - TS_{\mathrm{cl.}},$$
and show that this satisfies $\partial F/\partial T = -
S_{\mathrm{cl.}}$.  This quantity can be identified as the
thermodynamic \textbf{free energy}.

\section*{Further Reading}

\begin{itemize}
\item E.~T.~Jaynes, \textit{Information theory and statistical
  mechanics}, Physical Review \textbf{106}, 620 (1957).
\label{cite:jaynes}

\item E.~T.~Jaynes, \textit{Information theory and statistical
  mechanics. ii}, Physical Review \textbf{108}, 171 (1957).
\label{cite:jaynes2}
\end{itemize}

\end{document}
